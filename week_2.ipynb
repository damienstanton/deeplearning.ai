{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "- Note the differences between the squared loss func one would use in _linear regression_, and why this gets trapped in local minima in the case of binary classification (which is the use case here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\hat{y} = P(y = 1 | x)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "0\\le\\hat{y}\\le1\\ and\\ x\\in\\mathbb{R}^{n_x}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "parameters:w\\in\\mathbb{R}^{n_x}, b\\in\\mathbb{R}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "output:\\hat{y}=\\sigma(w^{T}+b)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sigmoid func above expands to\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "this gives the loss func\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{L} = (\\hat{y},y) = (y_{log}\\hat{y} + (1 - y)_{log}(1-\\hat{y})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "and the cost func\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{J}(w, b) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- the goal is to find a convex func via parameters _w_ and _b_ that minimizes `J(w, b)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rough algorithm:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "Loop:\n",
    "\\quad w := w = \\alpha\\frac{\\delta J(w)}{\\delta w}\n",
    "\\newline\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "where alpha is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this benchmark shows the perf increase from vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized: 1.0488033294677734ms\n",
      "unvectorized (for loop): 416.20612144470215ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(\"vectorized: {}ms\".format(1000*(toc-tic)))\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(\"unvectorized (for loop): {}ms\".format(1000*(toc-tic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions to note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s*(1-s)\n",
    "\n",
    "def im2vec(img):\n",
    "    length = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    depth = img.shape[2]\n",
    "    return img.reshape(length*width, depth)\n",
    "\n",
    "def normalize_rows(x):\n",
    "    return x/np.linalg.norm(x, axis=1, keepdims=True)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "def l1_loss(y_hat, y):\n",
    "    return np.sum(abs(y-y_hat))\n",
    "\n",
    "def l2_loss(y_hat, y):\n",
    "    y_diff = y-y_hat\n",
    "    return np.sum(np.dot(y_diff, y_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few interesting forumulae from the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$$\n",
    "$$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$$\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
